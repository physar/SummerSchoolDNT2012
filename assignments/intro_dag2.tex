\documentclass[a4paper]{article}

\usepackage{hyperref}

\title{\textbf{Robotics Summerschool Juli 2012} \\ Opdrachten Dag 2}
\author{Dutch Nao Team - \url{http://dutchnaoteam.nl}}
\date{}

\begin{document}
\maketitle

\section{Introductie}
Welkom bij de Summerschool Robotics.\\
\\
De nodige code voor deze opdracht kun je downloaden van:\\ \url{http://ducthnaoteam.nl/summerschool/dag2_start.zip}\\
\\
Unzip de code in de map \textit{SummerschoolRobotics2012} op je desktop. Je kunt nu beginnen met de tweede opdracht van deze summerschool. \\
\\
veel plezier en succes!\\
\\
Het Dutch Nao Team.\\
\\
\textbf{Wireless informatie:}\\
Router:  Dutch Nao Team Robolab\\
Wachtwoord: DnT10RuleS\\
Deze router is voor de mensen, zittend het dichts bij de lift.\\
\\
Router:  Dutch Nao Team NAO\\
Wachtwoord: sam+moos\\
Deze router is voor de mensen, zittend het dichts bij de glazen wand


\tableofcontents

\newpage


\section{Opdrachten}
Voor de vision opdrachten maken we gebruik van Open CV \footnote{Voor documentaties, zie: \\ \url{http://opencv.willowgarage.com/documentation/python/index.html}}. Dit is een Computer Vision library die door meerdere programmeertalen kan worden gebruikt voor beeld verwerking.

\subsection{Framework update}
Om de robot de weg te laten vinden, moet deze gebruik maken van zijn camera. Een Nao heeft 2 camera's in zijn hoofd zitten: in zijn mond en in zijn voorhoofd. Wij zullen enkel de camera in de mond gebruiken.

Net als bij motions, wordt de camera ook via een proxy aangeroepen. Door \textit{self.vidProxy = ALProxy( ``ALVideoDevice", self.ipadress, 9559 )} toe te voegen aan de \textit{setProxies()} functie krijgen we de mogelijkheid om te camera uit te lezen. Voordat we een foto maken moeten er nog enkele camera parameters worden ingesteld. Dit wordt gedaan door de \textit{cSubscribe()} functie in de \textit{tools}-module: voeg daarom deze module toe aan \textit{config.py} (de tools-module staat in de \textit{lib} map).

Het is aan te raden om voor deze dag een nieuw main-module te maken. Zorg ervoor dat de nieuwe main-module, de globals, motions en tools-module kan gebruiken. Initializeer daarna in de \textit{start()}-functie de proxies, motions en de camera parameters.

Test alle code en los eventuele errors op.

\subsection{Blob detectie}
De eerste stap in het navigeren door een doolhof is het vinden van een blob: een gebied in een plaatje dat een bepaalde kleur heeft. In de komende secties zul je je eigen blob-vinder maken.

\subsubsection{Een foto maken en opslaan}
Met behulp van de \textit{getSnapShot()} functie in de tools-module kun je een foto maken met de onderste camera van de robot. Deze functie geeft naast de foto ook extra gegevens terug over de positie van het hoofd. Het eerste argument van de geretourneerde waarde van \textit{getSnapShot()} is het plaatje.

Een gemaakt plaatje kan ook worden opgeslagen. Dit wordt gedaan door de functie \textit{saveImage(name, img)} aan te roepen in de tools-module. Hierin is \textit{name} de bestandsnaam van het plaatje inclusief de bestands-extensie (png of jpg). \textit{img} is het plaatje zelf.

Maak een plaatje en sla dit op. Het opgeslagen bestand kun je vinden in de zelfde map als waar je \textit{config.py}-bestand staat.

Wanneer je het plaatje opent, zul je zien dat de kleuren niet kloppen. Dit klopt, wij gebruiken namelijk de HSV-kleuren\footnote{Meer informatie over HSV kun je vinden op: \\ \url{http://nl.wikipedia.org/wiki/HSV_(kleurruimte)} } representatie in plaats van de wat bekendere RGB-representatie.

\subsubsection{Image filtering}
Een volgende stap in het proces is het filteren van een kleur. Hiervoor moet een functie worden gemaakt die een snapshot en enkele kleur-parameters meekrijgt en een gefilterd zwart/wit plaatje terug geeft. In dit gefilterde (binaire) plaatje staan witte pixels voor pixels welke in de meegegeven foto voldoen aan onze kleur-eisen en zwarte pixels staan voor pixels die hier niet aan voldoen.

Voordat code kan worden geschreven moet een module worden gemaakt waar straks de code in komt te staan (de \textit{vision}-module). Maak deze module (als het nog niet bestaat) en registreer het. Zorg er ook voor dat je in je module openCV importeert (\textit{import cv, cv2}) aangezien we deze library gaan gebruiken. Test je systeem op errors.

In de zojuist gemaakte module kan nu de filter-functie worden toegevoegd: \textit{filterImage(self, img, minHSV, maxHSV)}.
Als input krijgt het een plaatje mee en twee lijsten met elk 3 waardes (minimum en maximum waardes van de Hue, Saturation en Value van een kleur). Pixels in het plaatje die binnen deze kleur-grenzen vallen zijn pixels waarin we interesses hebben. Dit is te vergelijken met bijvoorbeeld een beschrijving van elke kleur die minimaal lichtblauw is en maximaal donkerblauw. Op deze manier kunnen we alle blauwe kleuren makeren en daarmee vinden we de pixels die van ons interessant zijn.
Om OpenCV te kunnen laten werken met de kleur-definities, moeten we het omzetten in een Scaler:

\noindent \line(1,0){100}
\begin{verbatim}
minScaler = cv.Scaler(minHSV[0], minHSV[1], minHSV[2], 0)
\end{verbatim}
\noindent \line(1,0){100}
\\\\
Het omzetten moet voor beiden lijsten (\textit{minHSV} en \textit{maxHSV}) worden gedaan.

De filter-functie maak een nieuw plaatje, om dit te kunnen doen moet geheugen worden vrijgemaakt:

\noindent \line(1,0){100}
\begin{verbatim}
resultImg = cv.CreateImage(size, cv.IPL_DEPTH_8U, 1)
\end{verbatim}
\noindent \line(1,0){100}
\\\\
Hierin is \textit{size} gelijk aan \textit{(320,240)}, de grootte van de gemaakte foto. \textit{cv.IPL\_DEPTH\_8U} geeft aan dat elke pixel uit het plaatje een 8-bit unsigned waarde heeft. Het aantal dimensies (kleuren) van het plaatje is 1: een pixel is enkel zwart of wit.

We kunnen nu OpenCV gebruiken om een `raw' zwart-wit plaatje te maken van onze foto:

\noindent \line(1,0){100}
\begin{verbatim}
cv.InRangeS(img, minScaler, maxScaler, resultImg)
\end{verbatim}
\noindent \line(1,0){100}
\\\\
Zorg ervoor dat (doormiddel van \textit{return}) de main-module straks het gefilterde plaatje terug krijgt.
Sla de code op en roep vanuit de main-module de functie aan met een kleur naar keuze

Hoogst waarschijnlijk zul je vele witte vlekken zien in plaatje: ruis. Om dit weg te halen moeten we het plaatje ``smoothen": de overgang tussen zwart/wit maken we minder `hard'. Dit wordt gedaan door steeds een aantal pixels naast elkaar te pakken en daar een gemiddelde waarde van te berekenen. Random witte pixels zullen daardoor een zwarte kleur krijgen en dus wegvallen. De volgende functie doet dit: \textit{cv.Smooth(resultImg, resultImg, cv.CV\_MEDIAN, 1)}. \textit{cv.CV\_MEDIAN} geeft het type ``smoothing" aan en het getal hoeveel naastelkaar gelegen pixels steeds worden gebruikt per berekening. Test een paar verschillende waardes en bepaal welk getal het beste resultaat geeft. \footnote{Op \url{http://opencv.willowgarage.com/documentation/python/image_filtering.html\#smooth} kun je meer vinden over de mogelijke opties van smoothing.}

Als laatste moet het het resulterende plaatje worden omgezet met behulp van het commando: \textit{resultImg = cv.GetMat(resultImg)}. Deze functie zorgt ervoor dat we straks verdere bewerkingen op het plaatje kunnen doen.

\subsubsection{Hough cirkels}
Nu we een gefilterd plaatje krijgen, kunnen we proberen cirkels te zoeken. Hiervoor is een nieuwe functie nodig in onze vision-module. Deze functie heeft (naast \textit{self}) slechts 1 argument: een binair plaatje (\textit{findCircle(self, img)}

Cirkel detectie doen we doormiddel van een speciaal algorithme: de Hough-transform\footnote{voor meer info check: \url{http://en.wikipedia.org/wiki/Hough_transform}}. Dit algorithme komt met OpenCV geleverd. Om de Hough-transform te kunnen toepassen moet het gegeven plaatje worden omgezet in een array via \textit{numpy} (een python module): \textit{img = numpy.asarray(img)}. Uiteraard moet numpy natuurlijk eerst worden geimporteerd om dit mogelijk te maken (\textit{import numpy}).

De volgende stap is het toepassen van de Hough-transform, hier komen echter veel verschillende parameters bij kijken. Om ervoor te zorgen dat je niet te veel tijd bent aan het tweaken hiervan geven wij je de ``juiste" parameters:

\noindent \line(1,0){100}
\begin{verbatim}
dp = 2
minD =120
p1 = 255
p2 = 27
minS = 8
maxS = 300
circles = cv2.HoughCircles(img, cv.CV_HOUGH_GRADIENT, dp, minD, None, p1, p2, minS, maxS)
\end{verbatim}
\noindent \line(1,0){100}
\\\\
\textit{dp} geeft aan hoe nauwkeurig moet worden gezocht (in ons geval om de 2 pixels), \textit{minD} geeft aan hoeveel pixels minimaal tussen 2 blobs moet zitten, \textit{p1} en \textit{p2} zijn parameters voor een dieper-liggend proces en \textit{minS} en \textit{maxS} geeft aan wat de minimale, danwel de maximale grote van een blob is in pixels.
De output is een lijst met daarin data over de gevonden cirkels (aantal, radius, pixel-center), of \textit{None} indien niks gevonden is.

Sla je code op en test de \textit{HoughCircles} functie. Print de resultaat coordinaten in de Command Prompt. 

Aan de hand van de geprintte data, kun je nu een systeem maken welke \textit{None} terug geeft als geen cirkel is gevonden of een lijst met coordinaten als er wel cirkels zijn gevonden.

Tip: Met behulp van het commando: \textit{cv.Circle(img, center, radius, (0, 0, 255), 3, 8, 0)} kun je een cirkel met centrum \textit{center} en straal \textit{radius} teken op het plaatje img. Wanneer je het plaatje opslaat kun je visueel zien waar het algorithme denkt dat de blobs zitten.

\subsubsection{Meerdere blobs}
Nu we blobs kunnen herkennen en berekenen waar ze in een plaatje zitten, kunnen we meerdere blobs combineren.

Hiervoor is een functie nodig (in de vision-module, \textit{getBlobsData(self, img)}) die een plaatje inlaad (geven door de main-module, via \textit{getSnapshot()}), die voor elke kleur blob detectie doet en vervolgens zoekt  naar Hough-Circles. De functie moet 2 outputs geven, de eerste is het aantal gevonden blobs, de tweede is een lijst met daarin de coordinaten (op volgorde) van de paarse, blauwe en oranje blob. Als er geen coordinaten van een blob zijn gevonden dan staat er \textit{None} op deze plek. 

\subsection{Landmark Detectie}
Gefeliciteerd, als je hier ben aangekomen zul je een werkende vision module hebben die blobs kan vinden.

De volgens stap is de mapping van de gevonden blobs naar een landmark en de berekening van de afstand en de hoek tot de landmark.

We beginnen met het berekenen van de afstand tot de landmark: je wilt de robot naar een landmark toe laten lopen en stoppen zodra hij op een bepaalde afstand van de landmark is. Dit is simpel te doen door de afstand tussen de verschillende blobs te berekenen. Zoals je bij de vorige opdracht gezien hebt is het vision-systeem gevoelig voor ruis. Daarom bereken je een gemiddelde afstand tussen de blobs. Hiervoor moet de functie \textit{calcAvgBlobDistance(self, blobList} worden gemaakt in de \textit{vision} module. Deze krijgt als input de lijst met blob gegevens. Als output wordt de gemiddelde afstand (in pixels) tussen de gevonden blobs terug gegeven. Bij 1 (of 0) blobs kan er geen afstand worden berekent, en wordt er \textit{None} geretouneerd.

Voor het berekenen van de hoek naar een landmark, moet bekend zijn wat het midden van de landmark is. De functie die dit doet is \textit{calcMidLandmark(self, blobList)}. Als input krijgt het de lijst met gevonden blob-data en als output geeft het een x/y pixel positie die het midden aangeeft van de set van de gevonden blobs. Indien geen blobs zijn gevonden moet geeft de functie \textit{None} terug.

Wanneer het midden van de set van blobs bekend is, kan de hoek ten opzichte van de robot worden berekend (\textit{calcAngleLandmark(self, center)}). Hiervoor gebruik je de volgende informatie:
De kijkhoek van de camera is ongeveer 70 graden (1.22 radialen). Een plaatje is 320 pixels breed, wat betekent dat een enkele pixel een hoek heeft van ongeveer 0.0038 radialen (1.22/320). Het midden van een plaatje heeft een hoek van 0 en een x/y pixel-positie van 160/120. \textit{None} moet worden geretouneerd wanneer geen blobs zijn gevonden en daarmee ook dus geen hoek kan worden berekend.

\newpage

Als laatste stap moet een functie (\textit{findSignature(self, blobList)}) worden gemaakt die bepaald wat de signature is van de geobserveerde landmark. De signatures zijn als volgt verdeeld:

\begin{enumerate}
\item - Blauw boven, finish
\item - Blauw rechts, rechtsaf
\item - Blauw links, linksaf
\end{enumerate}

Indien een landmark niet kan worden bepaald, is de signature gelijk aan -1.

\subsection{Navigatie}

Het is nu mogelijk, gegeven een snapshot, om blobs te vinden, de afstand tussen de blobs te berekenen (en daarmee de afstand van de robot naar de landmark te schatten) en de hoek naar de gevonden blobs te berekenen. Verder kan (indien er alle blobs zijn gevonden) de signature van de landmark worden achterhaald.

Met al deze informatie kan de functie \textit{calcDirection(self, blobList, blobDist, angle, signature)} worden gemaakt. Omdat dit geen vision-functie meer is, maar een behaviour moet een behaviour module worden aangemaakt.

De main module zal nu een snapshot maken, all vision functies aanroepen en de resultaten doorgeven aan de behaviour module. Deze module zorgt er dan voor, afhankelijk van de input, welke richting de robot op moet lopen of draaien.
\\

Hoe zorg je ervoor dat je robot het snelst door het doolhof gaat?\\

(extra notie: omdat je in een modulair systeem werkt kun je snel meerdere kopieen van je modules maken en laden. Hiermee kun je makkelijk kleine aanpassingen testen, zonder werkende code te verliezen)

\end{document}